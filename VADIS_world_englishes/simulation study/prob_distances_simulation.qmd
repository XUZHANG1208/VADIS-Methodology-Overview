---
title: "VADIS simulation studies"
author: "Jason Grafmiller<br>Benedikt Szmrecsanyi"
date: "Last updated: `r format(Sys.Date(), '%d %B, %Y')`"
format:
  html: 
    toc: true
    toc-location: left
    number-sections: true
    fig-width: 6
    fig-height: 4
    theme: 
      light: flatly
      dark: darkly
    mainfont: 'Roboto'
    fontsize: 100%
    df-print: kable
    code-fold: true
    code-copy: true
    code-tools:
      source: true
      toggle: false
      caption: "code"
    css: "custom_bib.css"
    include-in-header: roboto_serif.html
    include-after-body: footer.html
    embed-resources: true
knitr:
  opts_chunk: 
    cache.path: "../cache/"
    # fig.path: "../figures/"
    tidy: styler
execute:
  echo: false
  warning: false
  message: false
  cache: false
tbl-cap-location: top
fig-cap-location: bottom
filters:
  - lightbox
lightbox: auto
bibliography: [vadis_refs.bib, r_packages.bib]
csl: ../libs/unified-style-linguistics.csl
---



<!-- <style> -->
<!-- img { -->
<!--     display: block; -->
<!--     margin-left: auto; -->
<!--     margin-right: auto; -->
<!-- } -->
<!-- </style> -->

This report present the results of a set of simulation studies used to validate methods applied in the Variation-Based Distance & Similarity Modeling (**VADIS**) method introduced by @szmrecsanyi_variationbased_2019. The VADIS method builds upon techniques in comparative sociolinguistics and dialectometry for quantifying the similarity between varieties and dialects as captured by correspondences among the ways in which language users choose between different ways of saying the same thing. For details of the method and theoretical motivation see @szmrecsanyi_variationbased_2019. The `{VADIS}` R package can be found at <https://github.com/jasongraf1/VADIS>. 

```{r}
#| label: setup
#| include: false
pacman::p_load(
  here,        # for file path management
  tidyverse,   # for data wrangling and plotting: dplyr, ggplot2, etc.
  data.table,  # for data.tables (*much* faster data wrangling)
  knitr,       # for working with markdown docs

  patchwork,   # for combining ggplots
  ggrepel,     # for nicely annotating points with text 
  ggdendro,    # for tree dendrograms in {ggplot2}
  dendextend,  # supplement to {ggdendro}
  extrafont,   # for fonts in figures
  ggpubr,      # for ggplot themes
  ggsci,       # for ggplot color themes
  plotly,      # for interactive plots
  
  hypr,        # for calculating custom factor contrast codings
  lme4,        # for mixed-effects models
  lmerTest,    # significance tests for GLMMs
  optimx,      # for lme4 optimizer
  MuMIn,       # for r.squaredGLMM
  car,         # for recoding
  performance, # for evaluating model
  parameters,  # for summarizing model
  effects,     # get partial effects from models
  Hmisc,       # for somers2()
  
  ranger,      # for fast random forests
  phangorn,    # for neighborNets
  VADIS        # for VADIS analysis
)

library(rgl)
library(kableExtra)

theme_set(theme_minimal())
```


# VADIS in a nutshell

The VADIS method is designed to measure the degree of (dis)similarity among "variable grammars" [@tagliamonte_variationist_2012] of different dialects or varieties, where a variable grammar is understood as the set of constraints (a.k.a. predictors or "conditioning factors") governing the choice between two or more linguistic variants. Variants can be individual lexical items (*sneakers* vs. *trainers* vs. *tennis shoes*), grammatical constructions (*give me the book* vs. *give the book to me*), or phonetic realizations of a particular phoneme (e.g. [ʊ] vs. [ʌ] pronunciations of the <span style="font-family:Times New Roman, Times, serif">STRUT</span> vowel).  

The method takes inspiration from Comparative Sociolinguistics, which evaluates the relatedness between varieties and dialects based on how similar the conditioning of variation is in these varieties [@poplack_african_2001; @tagliamonte_comparative_2013]. Comparative sociolinguists rely on three lines of evidence to determine relatedness:

1. Are the same constraints significant across varieties? 
2. Do the constraints have the same strength across varieties? 
3. Is the relative explanatory importance of the constraints similar?

Similarity assessed via this approach is often interpreted as historical and genetic relatedness in variationist studies [e.g. @poplack_african_2001; @claes_cognitive_2016; @childs_variation_2017], though the approach has potential applications for investigating language contact phenomena [e.g. @ortin_transfer_2019] as well as individual level variation [e.g. @blaxter_each_2019]. VADIS draws inspiration from this literature and adapts the comparative sociolinguistics method so that it can be applied to datasets sampling (a) more than a handful dialects or varieties, and (b) more than one variation phenomenon at a time. This is accomplished through more rigorous quantification of the variability among the representative variable grammars.


## The analysis pipline

In practice, applying the method to a given alternation consists of the following steps:

**VADIS Step 1.** Identify the set of constraints or predictors *P* that govern the choice among variants, based on prior research and/or theoretical knowledge of the variable in question. Create datasets of carefully selected observations of said variable(s) across different varieties following established principles and practices in variationist research. Annotate these datasets for the relevant features corresponding to the set of predictors.

**VADIS Step 2.** Fit a regression model (specifically mixed-effects logistic regression models) to each variety-specific dataset, where the model structure will depend on the set of predictors thought to influence the variable.

**VADIS Step 3.** Using the variety-specific regression models, determine cross-variety similarity based on predictor significance. In this step, the similarity between two varieties is proportional to the extent to which the varieties overlap in which predictors significantly regulate variant choice. Distance is measured as the squared Euclidean distance, divided by the total number of constraints.

**VADIS Step 4.** Using the variety-specific regression models, determine cross-variety similarity based on predictor effect size and direction. In this step, the similarity between two varieties is proportional to the (Euclidean) distance between the two models' coefficient estimates. These distances are then scaled to fall between 0 and 1 [see @szmrecsanyi_variationbased_2019, 5 for details].

**VADIS Step 5.** Fit a random forest model to each variety-specific dataset, where the model structure will include (as much as possible) the same predictors as in Step 2.[^note1]

**VADIS Step 6.** Using the variety-specific random forest models, determine cross-variety similarity based on the relative predictive importance of the predictors. In this last step, the similarity between two varieties is proportional to the rank correlation between the two models' predictor importance rankings. 

The method provides two main outputs for assessing similarity among varieties: a set of 3 distance matrices, scaled from 0 to 1, reflecting the pairwise distances between datasets based on measures from each of the 3 lines of evidence; and 3 sets of similarity scores which reflect the average similarity of each variety to all others (calculated as 1 - the average distance). Variations of this method were used by @grafmiller_mapping_2018, and the method in full is introduced and described in @szmrecsanyi_variationbased_2019. 

We exemplify the method with the two tables below. @tbl-sz-tab2 shows a table of hypothetical coefficients from regression models of 3 varieties, similar to tables of factor weights in traditional variationist work. 

|            | Variety A | Variety B | Variety C |
|------------|----------:|----------:|----------:|
| Constraint | -2.10     | -1.50     | 1.20      |
| Constraint | -1.30     | -1.60     | -1.20     |
| Constraint | 0.75      | -0.05     | 0.63      |
| Constraint | 0.69      | 0.80      | 2.20      |
| Constraint | -0.92     | -1.0      | -0.79     |

: Table of predictor weightings (logit scaled) for 5 simulated varieties. From Szmrecsanyi et al. (2019:5) {#tbl-sz-tab2}

@tbl-sz-tab3 shows a matrix of pairwise distances derived from the first table, where greater values reflect greater dissimilarity between two varieties (0 indicates identical varieties). Such distance matrices work essentially like distance grids in road atlases, which specify geographic distances between locations.

|           | Variety A | Variety B | Variety C |
|-----------|:---------:|:---------:|:---------:|
| Variety A | 0         |           |           |
| Variety B | 1.05      | 0         |           |
| Variety C | 3.63      | 3.15      | 0         |

: Table of predictor weightings (logit scaled) for 5 simulated varieties. From Szmrecsanyi et al. (2019:5) {#tbl-sz-tab3}

Distance matrices of the kind above are the basic inputs of classical dialectometry, and are extremely useful for visualizing similarities in a number of ways. For example, Szmrecsanyi et al. (2019) use the VADIS method to derive a distance matrix from the various predictor importance rankings for the particle placement alternation in nine English varieties (@fig-sz-fig1). 

```{r}
#| label: fig-sz-fig1
#| fig-cap: "Figure 1 from Szmrecsanyi et al. (2019): VADIS distance matrix for the 3rd line of evidence in the particle placement alternation in 9 varieties of English. Scores range between 0 (maximal similarity) and 1 (maximal dissimilarity)"
#| fig-width: 7
#| fig-height: 6
include_graphics(here("figures", "sz_gr_ro_Figure1.png"))
```

From this matrix they create a multidimensional scaling plot to map the relative similarity in these rankings across the nine varieties, were greater proximity on the MDS plot represents greater similarity with respect to the effects of constraints on particle placement (@fig-sz-fig2)).

```{r}
#| label: fig-sz-fig2
#| fig-cap: "MDS representation of 3rd line distances for the particle placement alternation. Distances between data points in plot are proportional to probabilistic grammar distances between varieties. From Szmrecsanyi et al. (2019)"
#| fig-width: 5
#| fig-height: 4
include_graphics(here("figures", "sz_gr_ro_Figure2.png"))
```

The key idea is that differences in the variable grammars across varieties can be examined more easily, and more holistically, than they traditional sociolinguistic methods via such techniques. In standard Comparative Sociolinguistics studies the results of separate models are compared mainly by visual inspection of tables of model outputs [see e.g. @tagliamonte_comparative_2013, 135, 145], and patterns of covariation across varieties can be difficult to discern. VADIS builds upon this approach by quantifying such patterns in ways that can then be used to model similarities among varieties more robustly. As in traditional comparative approaches, this method is holistic in that it focuses less on differences among specific predictors, and more on how the systems as a whole vary across varieties. The method certainly does not preclude closer investigation of individual predictors, and indeed we recommend such detailed analysis. Rather the VADIS method is intended to provide an alternative, "bird's eye view" of the patterns across different varieties' variable grammars.   


## Validating the method

The following report is guided by three main questions:

**Question 1.** How valid are the measures and representations of grammatical distances? In other words, do the measures of similarity obtained via the VADIS method accurately reflect the true degree of similarity among grammars?  

**Question 2.** Can we determine a reasonable range of small, medium, or large degrees of grammatical similarity?

**Question 3.** Can we capture the uncertainty in our underlying grammatical models in the VADIS output?

To answer these questions we simulate realistic datasets of the kind common in comparative sociolinguistic studies, and apply the VADIS method to them. These simulated datasets are specifically designed to vary in the degree of similarity among them, and thus allow us to test the validity and reliability of the method in a hypothetical scenario. We present the procedures for creating the simulated datasets in the next section, followed by a brief description of the modeling procedures. We then present the results of the VADIS analysis in the remaining sections.  


# Simulation overview

Our aim is to create simulated datasets in which we explicitly control how similar the variable grammars are be to one another, apply the VADIS modeling to them, and then examine how these differences are captured in the VADIS output. The basic idea is that more similar grammars should have higher similarity scores and cluster closer together in multidimensional scaling (MDS) and/or clustering representations, but we can also make more specific predictions based on how we construct the simulated grammars. 

The simulation analysis proceeds as follows: 

**Simulation Step 1:** We create variable grammars for a number of "varieties" based on set of dummy predictors with different constraint weightings that are predefined to be more or less similar to one another in specific ways.

**Simulation Step 2:** We create a set of simulated datasets for each "variety" representing different individual "speakers" within that variety. We generate a (binary) set of outcome variants for each dataset using the variable grammar for that respective variety. Each speaker in a variety is assumed to have a different baseline use of the outcome variants, which are captured in adjustments to the intercepts in the models used to generate the datasets.

**Simulation Step 3:** We fit separate regression models and random forest models to each dataset, and extract the respective statistical metrics for the VADIS method, i.e. the 3 'lines of evidence'. These are the fixed effects coefficients and their significance measures for the regression models (lines 2 and 1 respectively), and the permutation variable importances from the random forest models (line 3).

**Simulation Step 4:** We calculate the pairwise similarity scores for our datasets, from which we calculate the average score for each of line of evidence, as well as a combined score. We examine whether the coefficients accurately reflect with the degree of similarity in the simulated grammars. 

**Simulation Step 5:** We use the distance matrices derived from the models to explore similarities among the datasets/varieties with dimension reduction techniques (MDS) and clustering methods. We then examine whether the predicted patterns are apparent in the visualizations.

**Note that all simulations are modeling binary outcomes**

```{r}
#| label: data
#| include: false
# use this to avoid re-running everything when compiling
main_dataset <- readRDS(here("data", "intercept_speaker_simulation_data.rds"))
mod_list <- readRDS(here("data", "intercept_speaker_glm_list.rds"))
line1 <- VADIS::vadis_line1(mod_list, path = F)
line2 <- VADIS::vadis_line2(mod_list, path = F)
rf_mod_list <- readRDS(here("data", "intercept_speaker_rf_list.rds"))
line3 <- VADIS::vadis_line3(rf_mod_list, path = F)

brm_list <- readRDS(here("data", "simulation_brm_list.rds"))
brm_line1 <- VADIS::vadis_line1(brm_list, path = F)
brm_line2 <- VADIS::vadis_line2(brm_list, path = F)

rf_mod_list2 <- readRDS(here("data", "cross_speaker_rf_list.rds"))
rm_line3 <- VADIS::vadis_line3(rf_mod_list2, path = F)

main_data_list <- readRDS(here("data", "simulation_datasets_stepped.rds"))
```


## Simulating probabilistic grammars

For the probabilistic variable grammars, we create a set of 8 test predictors which will correlate with a hypothetical binary outcome. All these predictors are designed to be representative of those found in natural language data. The predictors were designed as follows (more details below).  

- 3 binary factors
- 1 categorical factor with 3 levels
- 4 continuous predictors
    + 2 based on a normal distribution
    + 1 based on an F distribution (as approximated in e.g., normalized frequency distributions)
    + 1 based on a Poisson distribution (as in e.g., the number of words in a constituent)

Weightings for the predictors in all varieties are treated as adjustments on the logit scale, thus are comparable to coefficients in a logistic regression model. We use these predictors and weightings to create 5 distinct "variety grammars":

**Variety A.** This is a baseline grammar with a reasonable range of predictor effect sizes (weightings). 

**Variety B.** A mirror image of Variety A. Predictors have exact same effect size, but in the opposite direction. That is, weightings have identical absolute values but opposite signs. 

**Variety C.** A third variety with predictor weightings that are very different from both Varieties A and B, yet are still within a reasonable range.

**Variety D.** A variety with predictor weightings equivalent to those of Variety C, yet randomly increased or decreased by 20%. For example, if predictor *p* has an effect size of 1 in Variety C, it would be randomly assigned a value of either 1.2 or 0.8 in Variety D. For predictor *q* = .4 in Variety C, *q* in Variety D would be either 0.48 or 0.32. And so on. 

**Variety E.** 'Frankenstein' variety composed of 2 weightings taken from each of the other 4 varieties (2 from A, 2 from B, 2 from C, and 2 from D). 

The set of weightings are given in table below. These are the simulated probabilistic grammars governing use of a hypothetical (binary) variable.

```{r}
#| label: tbl-coef-tab
#| echo: false
#| tbl-cap: "Table of predictor weightings (logit scaled) for 5 simulated varieties"
tab <- data.frame(
  Var.A = c(2, -1.38, 0.56, -0.05, 0.3, 0.69, 1.8, -1.4, -0.69),
  Var.B = c(-2, 1.38, -0.56, 0.05, -0.3, -0.69, -1.8, 1.4, 0.69),
  Var.C = c(0.4, -1, 1, 0.03, -.3, .9, 0.4, 2.4, -0.9),
  Var.D = c(0.32, -1.20,  0.8,  0.024, -.36,  0.72, 0.48,  2.88, -1.08),
  Var.E = c(2, -1, 1, .05, -.3, .9, -.69, -1.4, -.08)
) %>% 
  `row.names<-`(c("Bin1 = Y", "Bin2 = Y", "Bin3 = Y", "Cont1", "Cont2", "Cont3", "Cont4", "Cat = 'b'", "Cat = 'c'"))
tab %>%
	kable() %>%
  kable_styling("hover")
```

In addition to these test predictors, we include 3 'noise' predictors which are not associated with the outcome. 

- 3 noise predictors 
    + 1 based on a continuous normal distribution 
    + 1 based on a Poisson distribution 
    + 1 binary factor

Again, these predictors are designed to be representative of features found in natural language data.


## Simulating variety datasets

For the simulation we created datasets of 2000 observations each. For each observation we randomly generate possible values for 11 features corresponding to the 8 test predictors and 3 noise predictors. The hypothetical features are distributed as so:

- 3 binary features
    + `bin1`: binary factor, *P*(1) = 0.35
    + `bin2`: binary factor, *P*(1) = 0.75
    + `bin3`: binary factor, *P*(1) = 0.6
- 1 categorical feature with 3 discrete values
    + `cat1`: 'a', 'b', 'c'; *P*(a) = .6; *P*(b) = .3; *P*(c) = .1
- 4 continuous features
    + `cont1`: normal distribution, *&mu;* = 100, *&sigma;* = 5
    + `cont2`: normal distribution, *&mu;* = 10, *&sigma;* = 2
    + `cont3`: F distribution, F(d1 = 1, d2 = 10)
    + `cont4`: Poisson distribution, Poisson(*&lambda;* = 2)
- 3 noise features (no relation to the outcome)
    + `noise1`: normal distribution, *&mu;* = 100, *&sigma;* = 5
    + `noise2`: Poisson distribution, Poisson(*&lambda;* = 2)
    + `noise3`: binary factor, *P*(1) = 0.4

The values of the 8 test features, together with the weightings in @tbl-coef-tab, are used to calculate the the probability that the outcome variant in a given observation is 1 (vs. 0). So, for a given observation *i*, 

$$P\left({\rm outcome}_i = 1\right) = \frac{1}{1+\exp(-X\beta)}, \\ $$
where

$$\begin{eqnarray*}
X\hat{\beta}= 
& & {\rm Intercept} \\
& & + \beta_1\:[{\rm bin1}] + \beta_2\:[{\rm bin2}] + \beta_3\:[{\rm bin3}] \\
& &  + \beta_4\:[{\rm cont1}] + \beta_5\:[{\rm cont2}] + \beta_6\:[{\rm cont3}] + \beta_7\:[{\rm cont4}] \\
& & + \beta_8\:[{\rm cat1 = b}] + \beta_9\: [{\rm cat1 = c}]\\ 
\end{eqnarray*}$$

For the purpose of the calculation, `bin1`, `bin2`, and `bin3` each have values of {0, 1}, and `cat1[a]` = 0, `cat1[b]` = 1, `cat1[c]` = 2.

For each hypothetical token, we then randomly assign an outcome of 0 or 1 from a binomial distribution using the probability of success calculated for that token from the formula above.  


## Simulating individual speakers {#sec-speaker-sim}

To take this simulation a step further, we simulate variation in preference at the level of individuals. The idea is that users of a given variety will share the same underlying grammar, but they may vary in their baseline preference for one variant or another. We do this by creating 15 datasets for each variety, where each dataset is generated using a different intercept. The intercepts are randomly sampled from a normal distribution with a mean of 0 and *SD* of .5, which gives us a range of values that is a reasonable approximation of natural variation where 95% of speakers will have a baseline proportion between .27 and .73. For each speaker we generate 2000 observations.

This results in a final tally of 75 distinct simulated datasets (5 varieties x 15 speakers), comprising 150000 total  observations. 


## Step-wise variation

Finally, to more fully examine relative degrees of difference, we create a series of "stepped" varieties in which we incrementally increase the variance in predictor effects among 'individuals' within each 'variety'. This results in a series of linguistic 'communities' with increasingly greater heterogeneity in individuals' variable grammars.    

We create these datasets by keeping the mean predictor effects constant across varieties, but we gradually increase the standard deviations of predictor effects among individuals within the variety. Specifically, at each step we increase the standard deviation of each predictor by 5% of its mean, and randomly sample values for each individual from a hypothetical normal distribution. For example, take a hypothetical predictor *p* with a mean effect size of 0.8 on the logit scale. At Step 0, we sample 15 values from a random normal distribution with mean of .8 and *SD* = 0, and assign those values to the 15 'individuals' in 'Variety 0'. In the next step, we again sample 15 values from a random normal distribution with mean of .8 but this time with a *SD* = .05 * .8 = `r round(.05*.8, 2)`, and assign those values to the 15 'individuals' in 'Variety 1'. For the next step we sample values from a random normal distribution with mean of .8 and *SD* = .1 * .8 = `r round(.1*.8, 2)`, and so on. We continue incrementing the multiplicative factor of the standard deviation by .05 up to a final step of .5, where we sample from a random normal distribution with mean of .8 and *SD* = .4. The results of this simulation are shown in @fig-stepwise-effects.

```{r}
#| label: fig-stepwise-effects
#| echo: false
#| fig-cap: "Predictor weightings (logit scaled) for 15 simulated individuals in 11 varieties with gradually increasing internal heterogeneity. Blue dots and lines represent the mean values +/- 1 SD."
#| fig-width: 7
#| fig-height: 8
set.seed(43214)
coefs <- c(
    bin1 = 2,
    bin2 = -1.38,
    bin3 = -.56,
    cont1 = -.03,
    cont2 = .3,
    cont3 = .69,
    cont4 = 1.1,
    cat1b = -1.2,
    cat1c = -.1
    )

lims <- data.frame(
  Step = paste0("Step_", 0:10) %>% rep(each = 15) %>% rep(times = length(coefs)),
  Predictor = rep(names(coefs), each = 15*11),
  mean = rep(coefs, each = 15*11), 
  lower = sapply(coefs, function(x){x - (abs(x) * seq(0,.5,.05))}) %>% 
    reshape2::melt() %>% pull(value) %>% rep(each = 15), 
  upper = sapply(coefs, function(x){x + (abs(x) * seq(0,.5,.05))}) %>% 
    reshape2::melt() %>% pull(value) %>% rep(each = 15)
)

data.frame(
  Step_0 = sapply(coefs, function(x) rnorm(15, mean = x, sd = 0*abs(x))) %>% round(2),
  Step_1 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .05*abs(x))) %>% round(2),
  Step_2 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .1*abs(x))) %>% round(2),
  Step_3 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .15*abs(x))) %>% round(2),
  Step_4 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .2*abs(x))) %>% round(2),
  Step_5 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .25*abs(x))) %>% round(2),
  Step_6 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .3*abs(x))) %>% round(2),
  Step_7 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .35*abs(x))) %>% round(2),
  Step_8 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .4*abs(x))) %>% round(2),
  Step_9 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .45*abs(x))) %>% round(2),
  Step_10 = sapply(coefs, function(x) rnorm(15, mean = x, sd = .5*abs(x))) %>% round(2)
) %>%
  pivot_longer(cols = everything()) %>% 
  mutate(
    Predictor = str_replace(name, "^.*\\.", ""),
    Step = as.factor(str_replace(name, "\\..*", "")),
  ) %>% 
  left_join(lims, by = c("Step", "Predictor")) %>% 
  mutate(Step = factor(Step, levels = paste0("Step_", 0:10))) %>% 
  ggplot() + 
  geom_point(aes(fct_rev(Step), value), alpha = .1) +
  coord_flip() +
  facet_wrap(~Predictor, scales = "free_x") +
  geom_pointrange(aes(fct_rev(Step), y = mean, ymin = lower, ymax = upper), color = "blue") +
  labs(x = "", y = "predictor effect size")
```

# Modeling

To evaluate the VADIS method we consider two common scenarios in comparative variationist research. The first scenario involves aggregating across individuals to create a model of the community grammar. This is by far the most common scenario in comparative variationist research, if for no other reason than that we often lack sufficient numbers of observations for each individual to reliably model them separately. So in this scenario we are looking at comparisons at the level of the community, or variety, more generally.    

The second scenario involves comparing the grammars of individual members of a linguistic community (or users of a variety). The assumption behind this second test case is that members of the same community to a large extent share the same underlying grammar, yet may vary with respect to their baseline preference for different variants. While this is an admittedly simplistic assumption, it is useful in that it provides us with a way of assessing the minimum distance that the method can be expected to detect in a real world situation. Put another way, it enables us to see just how much differences in the baseline proportion of the variants is likely to contribute to our measures of similarity. Comparing "individuals" this way provides an approximate ceiling to our similarity scores by providing an average similarity for data sampled from different individuals with the exact same underlying grammar. This helps address our second research question.   



## Similarities across communities

The full datasets for each variety consist of 30000 total observations (2000 tokens x 15 speakers), which is not a realistic dataset for typical variationist studies. Therefore, we down-sampled the data by randomly sampling 150 tokens from each speaker in each variety to give us 5 datasets of 2250 observations (150 tokens x 15 speakers) each. We then account for speaker variability by including a by-speaker intercept in our model structure.

```{r}
#| label: fmla1
f <- Variant ~ (1|Speaker) + bin1 + bin2 + bin3 + cat1 + cont1 + cont2 + cont3 + cont4 +
  noise1 + noise2 + noise3
```

For the first two lines of evidence, we fit Bayesian generalized linear mixed models with standardized inputs and reasonably informative normal priors (`normal(0,2)`) for the fixed effects [@gelman_weakly_2008; @ghosh_use_2018]. Models are fit using the `{brms}` package [@burkner_brms_2017]. For the first line of evidence, which relies on statistical significance tests, we take a frequentist-like approach and simply use the 95% highest posterior density (HPD) interval. If the HPD interval for a given predictor does not include 0, the effect is treated as "significant".[^sig]

For the third line of evidence we fit random forest models using the `{ranger}` package [@wright_ranger_2017] and calculate the permutation based measure of predictor importance [@altmann_permutation_2010; @nicodemus_behaviour_2010a].[^rfnote] 

## Similarities across individuals

For examining similarities among individuals within varieties, simple generalized linear models (`glm`) without random effects were fit to each of the 75 datasets. The model structure was the same as above, minus the by-speaker intercept:  

```{r}
#| label: fmla2
f <- Variant ~ bin1 + bin2 + bin3 + cat1 + cont1 + cont2 + cont3 + cont4 +
  noise1 + noise2 + noise3
```

This same formula was used for the regression and random forest models. Separate, regression and random forest models were fit to each individual dataset.

# Representing grammatical similarity

We now turn to the result of applying the VADIS method to the simulated datasets. But before doing so, it's worth reflecting on what we expect to see. In a nutshell, we expect to find the following basic patterns:

- Varieties A and B should be quite distinct from one another, as their grammars are polar opposites.
- Varieties C and D should be quite similar to each other, due to the relatively small differences in their constraints, yet both they should be quite distinct from both A and B.
- Variety E should be different still from the others, as it was created from a combination of predictors from each variety 
- Inter-speaker variation within each variety should be considerably less than across varieties, with the possible exception of varieties C and D, since speakers differ only in their baseline frequencies. In other words, we expect speakers of the same variety to cluster tightly with one another. 

To validate the VADIS method, we expect to see these patterns emerge in the distance/similarity measures and  visualizations derived from those measures. Successful validation would result in clear sorting and/or clustering of our 'individuals' *within* varieties, as well as clear separation of (some of) those clusters *across* the varieties according to the patterns described above.

## Distance matrices

We start with a consideration of the pairwise distances between each of the 75 distinct 'individuals', and consider the distances as assessed via the three 'lines of evidence': statistical significance (Line 1), effect size and direction (Line 2), and predictor importance ranking (Line 3). We use a heatmap plot to represent the pairwise distance matrices for each line of evidence. 

### Line 1: Statistical significance

The first line asks: to what extent are the same predictors statistically significance across the two varieties? The distance matrix for this line (@fig-dist1) is derived from a table of binary values using the squared euclidean distance. The resulting values are then divided by the total number of predictors to arrive at values between 0 and 1. 

```{r}
#| label: func-heatmap

# Define a function for plotting a heatmap representing the pairwise 
# cosine distances for every token.
plot_dist_heatmap <- function(
  df, 
  title = "distance"){
  p <- df %>% 
    as.data.frame() %>% 
  plot_ly(
      x = ~label, 
      y = ~name,
      z = ~value, 
      type = "heatmap",
      text = ~df[, "label"],
      hoverinfo = "text",
      hovertext = paste(
        "X:", .$label,
        "<br>Y:", .$name,
        "<br>Distance:", round(.$value, 3))
      ) %>%
    layout(
      title = list(text = title, x = 0),
      xaxis = list(title = ""), 
      yaxis = list(title = "")
      )
  p                      
}
```


```{r}
#| label: fig-dist1
#| fig-cap: "VADIS distance matrix for the 1st line of evidence in comparison of 75 hypothetical individuals across 5 hypothetical varieties."
#| fig-width: 9
#| fig-height: 8

# Plot the grid in plotly 
line1$distance.matrix %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  rownames_to_column("label") %>% 
  pivot_longer(-label) %>% 
  mutate(
    label = case_when(
      str_detect(label, "var1") ~ str_replace(label, "var1", "A"),
      str_detect(label, "var2") ~ str_replace(label, "var2", "B"),
      str_detect(label, "var3") ~ str_replace(label, "var3", "C"),
      str_detect(label, "var4") ~ str_replace(label, "var4", "D"),
      str_detect(label, "var5") ~ str_replace(label, "var5", "E")
    ),
    name = case_when(
      str_detect(name, "var1") ~ str_replace(name, "var1", "A"),
      str_detect(name, "var2") ~ str_replace(name, "var2", "B"),
      str_detect(name, "var3") ~ str_replace(name, "var3", "C"),
      str_detect(name, "var4") ~ str_replace(name, "var4", "D"),
      str_detect(name, "var5") ~ str_replace(name, "var5", "E")
    )) %>% 
  plot_dist_heatmap(
    title = "Line 1: Squared Euclidean distance"
  )
```

The picture here suggest that this line of evidence alone is not very useful for distinguishing these hypothetical varieties. This illustrates one of the main limitations of this first line, which is that the distances are derived from a relatively small set of 11 binary values. Overall there is little information by which we can discriminate among the 75 datasets, and even a single change from one dataset to the next can have a substantial impact on the distance measure. With larger models containing perhaps dozens of predictors, all of which could potentially be significant, this line might be of more use (as tables of binary features are use in taxonomic studies). But such models are not realistic in variationist studies, so we'll need to triangulate these results with other lines of evidence.  

```{r}
#| label: fig-line1
#| fig-cap: "Binary significance values (1 = significant) for each predictor in each variety's model."
#| fig-width: 5
#| fig-height: 4
line1$signif.table |> 
  kable() |> 
  kable_styling()
```


### Line 2: Effect size

The second line asks: to what extent do the predictors' effects have the same size and direction across the two varieties?[^note4] The distance matrix for this line @fig-dist2 is derived from a table of regression coefficients (on the logit scale) using the euclidean distance. The resulting disctances can be further normalized to, e.g., fall between 0 and 1. Normalization in this way provides a baseline against which similarities can be compared, and allows (in principle) the results of one variable to be compared to other variables to assess cross-varietal similarity of different linguistic variables. However, we present the raw (unnormalized) distances here, in order to investigate similarities among varieties with minimal distortion. 

```{r}
#| label: fig-dist2
#| fig-cap: "VADIS distance matrix for the 2nd line of evidence in comparison of 75 hypothetical individuals across 5 hypothetical varieties."
#| fig-width: 9
#| fig-height: 8
line2$distance.matrix %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  rownames_to_column("label") %>% 
  pivot_longer(-label) %>% 
  mutate(
    label = case_when(
      str_detect(label, "var1") ~ str_replace(label, "var1", "A"),
      str_detect(label, "var2") ~ str_replace(label, "var2", "B"),
      str_detect(label, "var3") ~ str_replace(label, "var3", "C"),
      str_detect(label, "var4") ~ str_replace(label, "var4", "D"),
      str_detect(label, "var5") ~ str_replace(label, "var5", "E")
    ),
    name = case_when(
      str_detect(name, "var1") ~ str_replace(name, "var1", "A"),
      str_detect(name, "var2") ~ str_replace(name, "var2", "B"),
      str_detect(name, "var3") ~ str_replace(name, "var3", "C"),
      str_detect(name, "var4") ~ str_replace(name, "var4", "D"),
      str_detect(name, "var5") ~ str_replace(name, "var5", "E")
    )) %>% 
  plot_dist_heatmap(
    title = "Line 2: Euclidean distance"
  )
```

The picture here looks exactly like we want. The A and B varieties are least like one another, while the C and D varieties are most like one another. Variety E is different from all the others, though perhaps slightly more similar to C and D than A and B. Moreover, the *intra*-variety variability is very small compared to the differences across varieties as a whole. This is exactly as designed in the simulated data, though we note that such as high degree of intravarietal homogeneity is unlikely to show up in the real world. There is surely greater variability among individual language users than this simulation implies. 


### Line 3: Predictor importance

The third line asks: to what extent do the predictors' relative importance in the grammar? The distance matrix for this line (@fig-dist3) is derived from a table of random forest variable importance rankings, where distance is measured as 1 - the Spearman rank correlation *&rho;*. 

```{r}
#| label: fig-dist3
#| fig-cap: "VADIS distance matrix for the 3rd line of evidence in comparison of 75 hypothetical individuals across 5 hypothetical varieties."
#| fig-width: 9
#| fig-height: 8
line3$distance.matrix %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  rownames_to_column("label") %>% 
  pivot_longer(-label) %>% 
  mutate(
    label = case_when(
      str_detect(label, "var1") ~ str_replace(label, "var1", "A"),
      str_detect(label, "var2") ~ str_replace(label, "var2", "B"),
      str_detect(label, "var3") ~ str_replace(label, "var3", "C"),
      str_detect(label, "var4") ~ str_replace(label, "var4", "D"),
      str_detect(label, "var5") ~ str_replace(label, "var5", "E")
    ),
    name = case_when(
      str_detect(name, "var1") ~ str_replace(name, "var1", "A"),
      str_detect(name, "var2") ~ str_replace(name, "var2", "B"),
      str_detect(name, "var3") ~ str_replace(name, "var3", "C"),
      str_detect(name, "var4") ~ str_replace(name, "var4", "D"),
      str_detect(name, "var5") ~ str_replace(name, "var5", "E")
    )) %>% 
  plot_dist_heatmap(
    title = "Line 3: predictor ranking"
  )
```

As with Line 2, we find clearer distinctions between varieties than in Line 1, but the pattern is different from Line 2. Specifically we see suggestions of 3 main variety clusters: C and D, A and B, and E. This is not in line with the true patterns among varities, but it *is* what we should expect from this line of evidence.

Recall that varieties C and D were designed to be relatively similar to one another in a very particular way, i.e. by adjusting the effect sizes of the predictors by 20%, and so the relative ranking of predictors is not likely to vary much between (speakers of) the two varieties. Thus the two varieties are in fact qualitatively quite similar in their underlying grammars, and this is captured in the low distance scores.

The situation with varieties A, B, and E is different however. For one, why are A and B so similar, when we desinged them to be opposites? Again recall that varieties A and B were constructed to have predictors with *identical magnitudes but opposite directions*, which means that these varieties’ grammars are qualitatively very different. However, random forest predictor importance measures only assess the contribution of a predictor to the model’s overall ability to predict the outcome, and do not take into account the direction of the effects. Thus, in the random forest models for A and B, the predictors have the same relative importance in both varieties even though they have opposite effect directions.

Variety E is different from C and D, but very similar to A and B, which was not intended, but is not necessarily surprising given the combined nature of the set of constraints used to create the variety E grammar.

## Multidimensional scaling

```{r}
coef_df <- map_dfr(mod_list, coef) %>%
  mutate(
    Label = names(mod_list)
  ) %>%
  column_to_rownames("Label")
coef_dist <- dist(coef_df[, -1], method = "euclidean")
coef_mds <- cmdscale(coef_dist, eig = TRUE, k = 3)

# Make dataframe for plotting.
coef_mds_df <- as.data.frame(coef_mds[[1]])
names(coef_mds_df) <- c("x", "y", "z")
coef_mds_df <- coef_mds_df %>%
  mutate(
    Name = rownames(coef_mds_df),
    Variety = str_replace(Name, '\\w$', '') %>% factor(),
    Speaker = str_extract(Name, '\\w$') %>% factor(),
    )
levels(coef_mds_df$Variety) <- LETTERS[1:5]

######
line1 <- vadis_line1(mod_list, path = F)
sig_mds <- cmdscale(line1$distance.matrix, k = 3) %>% 
  as.data.frame() 
names(sig_mds) <- c("x", "y", "z")
sig_mds <- sig_mds %>%
  mutate(
    Name = rownames(sig_mds),
    Variety = str_replace(Name, '\\w$', '') %>% factor(),
    Speaker = str_extract(Name, '\\w$') %>% factor(),
    )
levels(sig_mds$Variety) <- LETTERS[1:5]

## set colors
cols <- RColorBrewer::brewer.pal(5, "Dark2")
names(cols) <- levels(coef_mds_df$Variety)
```

### Line 1: Statistical significance

Multidimensional scaling plots are an easy way to represent distances, and conversely similarities, visually. We'll start with plots from the first line of evidence, which considers the similarities in the statistical significance of the predictors across varieties. A 3D MDS plot (@fig-mds-3d-line1) shows that there is indeed some separation among the varieties in the ways we expect, but it is not very clean.

```{r eval = F}
#| eval: false
ggplot(sig_mds, aes(x, y, color = Variety)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_text(aes(label = Variety), size = 5) +
  scale_color_manual(guide = "none", values = cols) +
  theme_minimal() +
  ggtitle("Line 1: Significance similarities")
```

There is a lot of overlap between varieties A and B, and between varieties C and D. There's also a lot of overlap among individuals, reflected by the fact that many point are plotted overtop one another. 

```{r}
#| label: fig-mds-3d-line1
#| fig-cap: "3D MDS plot of similarities among varieties and speakers based on Line 1 (statistical significance)"
#| fig-width: 6
#| fig-height: 7
# Plot in 3 dimensions
sig_mds |>
  plot_ly() |>
  add_trace(
    x = ~x, y = ~y, z = ~z,
    type = "scatter3d", inherit = F,
    color = ~ Variety,
    colors = cols,
    mode = "markers",
    size = ~ rep(1, nrow(sig_mds)),
    marker = list(symbol = 'circle', sizemode = 'diameter'), sizes = c(8,8)
    ) |>
  add_text(
    x = ~x, y = ~y, z = ~z,
    text = ~ Speaker,
    color = ~ Variety,
    colors = cols,
    type = "scatter3d",
    mode = "markers",
    showlegend = FALSE) |> 
  layout(
    title = list(text = "Line 1: statistical significance", x = 0)
    )
```

<!-- ```{r} -->
<!-- library(scatterplot3d) -->
<!-- colors <- as.numeric(sig_mds$Variety) -->
<!-- with(sig_mds, -->
<!--      scatterplot3d( -->
<!--        x = x,  -->
<!--        y = y,  -->
<!--        z = z,  -->
<!--        pch = 16, -->
<!--        type = "h", -->
<!--        color = cols[as.numeric(sig_mds$Variety)], -->
<!--        angle = 60 -->
<!--        ) -->
<!--      ) -->
<!-- ``` -->

The overlapping points here illustrate one of the main limitations of this first line, which is that the distances are derived from a relatively small set of 12 binary values. Overall there is relatively little information by which we can discriminate among the 75 datasets, and even a single change from one dataset to the next can have a substantial impact on the distance measure. With larger models containing perhaps dozens of predictors, all of which could potentially be significant, this line might be of more use (e.g. as tables of binary features are use in taxonomic studies), but such models are not realistic in variationist studies, so we'll need to triangulate these results with other lines of evidence.  

Finally, in Variety E we notice a couple outlying speakers, b and g. It's not immediately clear why this is, though it is likely that the models of these particular datasets had some near separation problems, which affect the parameter estimates. 

### Line 2: Effect size & direction

Turning to the second line, effect size/direction, the results are much more promising (@fig-line2a-mds). We see clearly separated clusters representing the respective varieties. The patterns we describe above are largely borne out in the plot. But again we find the same two outliers in Variety E.

This is even easier to see in three dimensions.

```{r}
#| label: fig-line2a-mds
#| fig-cap: "3D MDS plot of similarities among varieties and speakers based on Line 2 (effect size)"
#| fig-width: 6
#| fig-height: 7
# Plot in 3 dimensions
coef_mds_df %>%
  plot_ly() %>%
  add_trace(
    x = ~x, y = ~y, z = ~z,
    type = "scatter3d", inherit = F,
    color = ~ Variety,
    colors = cols,
    size = ~ rep(1, nrow(sig_mds)),
    mode = "markers",
    marker = list(symbol = 'circle', sizemode = 'diameter'), sizes = c(8,8)
    ) %>%
  add_text(
    x = ~x, y = ~y, z = ~z,
    text = ~ Speaker,
    color = ~ Variety,
    colors = cols,
    type = "scatter3d",
    mode = "markers",
    showlegend = FALSE) %>% 
  layout(title = list(text = "Line 2: Effect size", x = 0))
```

We get a very nice picture here. The varieties are all clearly separated from one another, with the exception of C and D, which cluster relatively closely together. This was of course by design, as the predictor effect sizes of variety D were set to be equivalent to those of variety C but adjusted by 20%. Varieties A and B are maximally distant from each other in the plot space, and varieties C, D, and E are separated from these along different axes. This is exactly what we wanted to see based on how we designed the predictor effect sizes.


### Line 3: Importance ranking

Turning to the third line of evidence, we see results similar in robustness to those in line 2, but with a slightly different pattern (@fig-line3b-mds). There is clear separation among the varieties, similar to Line 2 , but there are some key differences that are worth examining further.

```{r}
rank_mds <- cmdscale(line3$distance.matrix, k = 3) %>% 
  as.data.frame() 
names(rank_mds) <- c("x", "y", "z")
rank_mds <- rank_mds %>%
  mutate(
    Name = rownames(rank_mds),
    Variety = str_replace(Name, '\\w$', '') %>% factor(),
    Speaker = str_extract(Name, '\\w$') %>% factor(),
    )
levels(rank_mds$Variety) <- LETTERS[1:5]
```

```{r}
#| label: fig-line3b-mds
#| fig-cap: "3D MDS plot of similarities among varieties and speakers based on Line 3 (importance ranking)"
#| fig-width: 5
#| fig-height: 4
# Plot in 3 dimensions
rank_mds %>%
  plot_ly() %>%
  add_trace(
    x = ~x, y = ~y, z = ~z,
    type = "scatter3d", inherit = F,
    color = ~ Variety,
    colors = cols,
    size = ~ rep(1, nrow(sig_mds)),
    mode = "markers",
    marker = list(symbol = 'circle', sizemode = 'diameter'), sizes = c(8)
    ) %>%
  add_text(
    x = ~x, y = ~y, z = ~z,
    text = ~ Speaker,
    color = ~ Variety,
    colors = cols,
    type = "scatter3d",
    mode = "markers",
    showlegend = FALSE) %>% 
  layout(title = list(text = "Line 3: Importance ranking", x = 0))
```

Rather than the 5 distinct clusters we see in @fig-line2a-mds, we find instead 2 loose clusters: a cluster of speakers of varieties A, B and E; and a somewhat looser cluster of speakers of varieties C and D. The two clusters are particularly interesting because they cluster together for different reasons, and they illustrate one of the limitations of the third line of evidence. 

Recall that varieties C and D were designed to be relatively similar to one another in a very particular way, i.e. by adjusting the effect sizes of the predictors by 20%, and so the relative ranking of predictors is not likely to vary much between (speakers of) the two varieties. Thus the two varieties are in fact qualitatively quite similar in their underlying grammars, and this is captured in the loose clustering in the MDS plot.

The situation with varieties A, B, and E is different however. Again recall that varieties A and B were constructed to have predictors with identical magnitudes but opposite directions, which means that these varieties' grammars are qualitatively very different. However, random forest predictor importance measures only assess the contribution of a predictor to the model's overall ability to predict the outcome, and do not take into account the direction of the effects. Thus, in the random forest models, the predictors have the same relative importance in both varieties even though they have opposite effect directions.     

Furthermore, importance rankings are often heavily skewed, with many predictors showing relatively little impact, and the rank ordering of weak predictors can vary randomly between datasets. This is illustrated in @fig-line3-varimps) which plots the mean predictor importances for each of the 5 varieties, with standard deviations. 

```{r}
#| label: fig-line3-varimps
#| fig-cap: Mean variable importance (with SD) for each variety
#| fig-width: 9
#| fig-height: 4
data_sum <- lapply(rf_mod_list, ranger::importance) %>%
  bind_rows() %>%
  mutate(Name = names(rf_mod_list)) %>% 
  pivot_longer(-Name) %>% 
  mutate(
    Variety = str_replace(Name, '\\w$', '') %>% factor(),
    Speaker = str_extract(Name, '\\w$') %>% factor(),
    ) %>% 
  group_by(name, Variety) %>% 
  summarise(
    mean = mean(value),
    upper = mean + sd(value),
    lower = mean - sd(value)
  )
levels(data_sum$Variety) <- paste0("Variety ", LETTERS[1:5])
data_split <- split(data_sum, data_sum$Variety)
plot_list <- lapply(
 seq_along(data_split),
 FUN = function(i){
   d <- data_split[[i]]
   ggplot(d,aes(fct_reorder(name, mean), mean)) +
   # geom_text(aes(label = Speaker)) +
   geom_pointrange(aes(ymin = lower, ymax = upper), color = cols[i]) +
   facet_wrap(~Variety,ncol = 2) +
   coord_flip() +
   labs(x = "", y = "") +
     theme(strip.text = element_text(face = "bold", size = rel(1.1)))
 })

(plot_list[[1]] + plot_list[[2]]) / (plot_list[[3]] + plot_list[[4]]) / (plot_list[[5]] + plot_spacer()) +
  plot_annotation(caption = 'permutation importance',
                  theme = theme(plot.caption = element_text(hjust = .5, size = rel(1.1))))

```

Note the differences in rankings among even the noise predictors. This random variability in the tails of the distributions is likely responsible for variability within varieties in @fig-line3b-mds). As with Line 1, the results for Line 3 illustrate the potential pitfalls of relying on a single line of evidence.

#### Putting it all together

We can consider the patterns of the combined lines of evidence by fusing the distance matrices derived from each line of evidence into a single 'average' distance matrix. @fig-fused-mds shows several perspectives of such averaged distances. 

```{r}
fused_dist <- analogue::fuse(line1$distance.matrix, 
                             line2$distance.matrix, 
                             line3$distance.matrix)
fused_mds <- cmdscale(fused_dist, k = 3) %>% 
  as.data.frame() 
names(fused_mds) <- c("x", "y", "z")
fused_mds <- fused_mds %>%
  rownames_to_column("Name") |> 
  mutate(
    # Name = rownames(rank_mds),
    Variety = str_replace(Name, '\\w$', '') %>% factor(),
    Speaker = str_extract(Name, '\\w$') %>% factor(),
    )
levels(fused_mds$Variety) <- LETTERS[1:5]
```

```{r}
#| label: fig-fused-mds
#| fig-cap: Perspectives on a 3D MDS of a distances derived from a matrix averaging over the three lines of evidence
#| fig-width: 7
#| fig-height: 6
p1 <- fused_mds %>% 
  ggplot(aes(x, y, color = Variety)) +
  geom_point(aes(shape = Variety)) +
  stat_ellipse(level = .9, linetype = "dashed", type = "norm") +
  stat_ellipse(level = .5, linewidth = 1, type = "norm") +
  ggtitle("Dimensions 1 x 2") +
  # ggthemes::scale_color_colorblind() +
  scale_color_manual(values = cols)
  

p2 <- fused_mds %>% 
  ggplot(aes(x, z, color = Variety)) +
  geom_point(aes(shape = Variety)) +
  stat_ellipse(level = .9, linetype = "dashed", type = "norm") +
  stat_ellipse(level = .5, linewidth = 1, type = "norm") +
  ggtitle("Dimensions 1 x 3") +
  # ggthemes::scale_color_colorblind() +
  scale_color_manual(values = cols)

p3 <- fused_mds %>% 
  ggplot(aes(y, z, color = Variety)) +
  geom_point(aes(shape = Variety)) +
  stat_ellipse(level = .9, linetype = "dashed", type = "norm") +
  stat_ellipse(level = .5, linewidth = 1, type = "norm") +
  ggtitle("Dimensions 2 x 3") +
  # ggthemes::scale_color_colorblind()+
  scale_color_manual(values = cols)

p1 + p2 + p3 + guide_area() + plot_layout(guides = "collect") + 
  plot_annotation(tag_levels = 'A', tag_suffix = ".") & 
  theme(plot.tag = element_text(hjust = 0)) 
```


## Clustering methods

Clustering methods offer alternative perspectives on the patterns among varieties, and are easy to implement. For example, @fig-hclust2 shows a hierarchical cluster analysis based on the Line 2 distance matrix. The analysis cleanly identifies the same 5 distinct clusters shown in the MDS plot above (@fig-line2a-mds)).

```{r}
#| label: fig-hclust2
#| fig-cap: Hierarchical clustering of datasets based on VADIS Line 2
#| fig-height: 9
dd.row <- hclust(coef_dist, "average") %>% 
  as.dendrogram()
ddata_x <- dendro_data(dd.row)

labs <- ggdendro::label(ddata_x) %>% 
  mutate(
    Variety = LETTERS[as.integer(str_extract(label, "\\d"))],
    label = str_replace(label, "var\\d", Variety)
  )

ggplot(segment(ddata_x)) +
geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(
    data = labs,
    aes(label = label, x = x, y = y, colour = Variety),
    hjust = 1) +
  scale_color_manual(values = cols) +
  coord_flip() +
  ggtitle("Line 2: effect size") +
  theme_dendro()
```

Similarly, a cluster analysis based on Line 3 (@fig-hclust3) also parallels the respective MDS plot, identifying two clear clusters: C & D and A, B & E. It also suggests a third cluster containing mostly datasets from Variety E. This cluster is also somewhat identifiable in the MDS plot. 

```{r}
#| label: fig-hclust3
#| fig-cap: Hierarchical clustering of datasets based on VADIS Line 3
#| fig-width: 5
#| fig-height: 9
dd.row <- hclust(line3$distance.matrix, "average") %>% 
  as.dendrogram()
ddata_x <- dendro_data(dd.row)

labs <- ggdendro::label(ddata_x) %>% 
  mutate(
    Variety = LETTERS[as.integer(str_extract(label, "\\d"))],
    label = str_replace(label, "var\\d", Variety)
  )

ggplot(segment(ddata_x)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(
    data = labs,
    aes(label = label, x = x, y = y, colour = Variety),
    hjust = 1) +
  scale_color_manual(values = cols) +
  coord_flip() +
  ggtitle("Line 3: predictive importance") +
  theme_dendro()
```

And, of course, we can do the same with the distances averaged from all 3 lines of evidence. We see a pretty clear separation of varieties, as we would hope.

```{r}
#| label: fig-hclust-fused
#| fig-cap: Hierarchical clustering of datasets based on all 3 lines
#| fig-width: 5
#| fig-height: 9
fused_dist <- analogue::fuse(line1$distance.matrix, 
                             line2$distance.matrix, 
                             line3$distance.matrix)

dd.row <- hclust(fused_dist, "average") %>% 
  as.dendrogram()
ddata_x <- dendro_data(dd.row)

labs <- ggdendro::label(ddata_x) %>% 
  mutate(
    Variety = LETTERS[as.integer(str_extract(label, "\\d"))],
    label = str_replace(label, "var\\d", Variety)
  )

ggplot(segment(ddata_x)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(
    data = labs,
    aes(label = label, x = x, y = y, colour = Variety),
    hjust = 1) +
  scale_color_manual(values = cols) +
  coord_flip() +
  ggtitle("Fused distance") +
  theme_dendro()
```

Other clustering methods, e.g. Neighbor nets [@bryant_neighbornet_2004] are becoming popular alternatives to traditional clustering methods as well [e.g. @dunn_structural_2008; @grafmiller_mapping_2018].



## Within variety similarity

We first look at the similarity among individuals within the same variety. To estimate similarities we take the distance matrices from each line of evidence and calculate the mean pairwise distance for each individual and subtract that value from 1. This provides an average similarity score for each individual ranging from 1 (maximal similarity, i.e. identity) to 0. In the same spirit, we can take the average pairwise distance across the entire variety to obtain a similarity score reflecting the degree of internal homogeneity within a given variety. Recall that the datasets for the five variety were created from 15 'individuals' with the same variable grammars, albeit with varying baseline frequencies of the outcome (intercepts). We thus expect the VADIS lines to return internal homogeneity scores that are quite close to 1. We interpret these values as the maximal degree of similarity, or conversely the minimal grammatical distance, that the method is capable of detecting, and therefore they represent an approximate practical ceiling (or floor) against which other values may be evaluated. 

For the first Line of evidence we find that the average degree of inter-speaker similarity is quite high across the five varieties (Table @tbl-line1-avg-dist-byVar)). This is not surprising considering the results presented above for the MDS plots. We don't expect to see much interpretable variability here, yet we do find a consistently high degree of variety-internal consistency.

```{r}
#| label: tbl-line1-avg-dist-byVar
#| tbl-cap: "Average speaker-to-speaker Similarity scores within each variety as estimated by Line 1 (statistical significance)"
line1_mean_dist <- data.frame(
  Variety = LETTERS[1:5],
  Mean = NA,
  Median = NA,
  SD = NA
)
variety_list1 <- vector("list", 5L)
for (i in seq_len(5)){
  var_line <- vadis_line1(mod_list[names(mod_list)[grepl(as.character(i), names(mod_list))]], path = FALSE)
  variety_list1[[i]] <- var_line
  line1_mean_dist[i, -1] <- c(
    1 - var_line$distance.matrix %>% mean() %>% round(3),
    1 - var_line$distance.matrix %>% median() %>% round(3),
    var_line$distance.matrix %>% sd() %>% round(3)
  )
}
names(variety_list1) <- paste0("var", LETTERS[1:5])
line1_mean_dist %>% 
  mutate(across(2:4, round, 3)) %>% 
  knitr::kable() |> 
  kable_styling("hover")
```

We find a similar degree of homogeneity for Lines 2 (@tbl-line2-avg-dist-byVar) and 3 ( @tbl-line3-avg-dist-byVar). 

```{r}
#| label: tbl-line2-avg-dist-byVar
#| tbl-cap: "Average speaker-to-speaker Similarity scores within each variety as estimated by Line 2 (effect size/direction)"
line2_mean_dist <- data.frame(
  Variety = LETTERS[1:5],
  Mean = NA,
  Median = NA,
  SD = NA
)
variety_list2 <- vector("list", 5L)
for (i in seq_len(5)){
  var_line <- vadis_line2(mod_list[names(mod_list)[grepl(as.character(i), names(mod_list))]], path = FALSE)
  variety_list2[[i]] <- var_line
  line2_mean_dist[i, -1] <- c(
    1 - var_line$distance.matrix %>% mean()  %>% round(3),
    1 - var_line$distance.matrix %>% median() %>% round(3),
    var_line$distance.matrix %>% sd()  %>% round(3)
  )
}
names(variety_list2) <- paste0("var", LETTERS[1:5])
line2_mean_dist %>% 
    mutate(across(2:4, round, 3)) %>% 
  knitr::kable() %>%
  kable_styling("hover")
```


```{r}
#| label: tbl-line3-avg-dist-byVar
#| tbl-cap: "Average speaker-to-speaker Similarity scores within each variety as estimated by Line 3 (importance ranking)"
line3_mean_dist <- data.frame(
  Variety = LETTERS[1:5],
  Mean = NA,
  Median = NA,
  SD = NA
)
variety_list3 <- vector("list", 5L)
for (i in seq_len(5)){
  var_line <- vadis_line3(rf_mod_list[names(mod_list)[grepl(as.character(i), names(mod_list))]], path = FALSE)
  variety_list3[[i]] <- var_line
  line3_mean_dist[i, -1] <- c(
    1 - var_line$distance.matrix %>% mean(),
    1 - var_line$distance.matrix %>% median(),
    var_line$distance.matrix %>% sd()
  )
}
names(variety_list3) <- paste0("var", LETTERS[1:5])
line3_mean_dist %>% 
    mutate(across(2:4, round, 3)) %>% 
  knitr::kable() %>%
  kable_styling("hover")
```

Overall we find that the average within-variety grammatical similarity between individual speakers, who vary only with respect to their baseline frequency, is around .9 for lines 1 and 3, and a bit lower, around .85 to .9 for line 2. We can combine the three lines to derive an overall similarity score representing the internal homogeneity for each variety.

```{r}
#| label: tbl-all-avg-dist-byVarr
#| tbl-cap: "Speaker-to-speaker Similarity scores within each variety averaged across the three lines."
bind_rows(line1_mean_dist, line2_mean_dist, line3_mean_dist) %>% 
  group_by(Variety) %>%
  summarise(
    mean = mean(Mean) %>% round(3),
    median = median(Median) %>% round(3)
  ) %>% 
    mutate(across(2:3, round, 3)) %>% 
  knitr::kable() %>%
  kable_styling("hover")
```

These datasets were constructed to simulate the minimal amount of grammatical variation that we could realistically expect to find within a homogeneous community of speakers. We note that even this simulation is an overly conservative one---it's very unlikely that any true community of users ever exhibits the degree of homogeneity simulated here. These findings suggest that pairwise similarity scores of .85 or above represent cases of grammars that are indistinguishable from one another in any meaningful sense.  


## Cross-variety similarity

We now turn to the case of cross-variety comparisons of the 5 "community" varieties, examining the different lines of evidence in turn. This scenario is more representative of work in standard Comparative Sociolinguistics. We reiterate the planned differences among the 5 varieties here:

**Variety A.** A baseline grammar with a reasonable range of predictor weightings. Weightings for all varieties are treated as adjustments on the logit scale, thus are comparable to coefficients in a logistic regression model.

**Variety B.** A mirror image of Variety A. Predictors have exact same effect size, but in the opposite direction. That is, weightings have equivalent absolute values but opposite signs. 

**Variety C.** A third variety with predictor weightings that are very different from both Varieties A and B, yet are still within a reasonable range.

**Variety D.** A variety with predictor weightings equivalent to those of Variety C, yet randomly increased or decreased by 20%. For example, if predictor *P* has an effect size of 1 in Variety C, it would have a value of either 1.2 or 0.8 in Variety D.

**Variety E.** A 'Frankenstein' variety composed of 2 weightings taken from each of the other 4 varieties (2 from A, 2 from B, 2 from C, and 2 from D).

As with the visualizations, we expect to find the following basic patterns:

- Varieties A and B should be quite dissimilar, as their grammars are polar opposites.
- Varieties C and D should be quite similar to each other, due to the relatively small differences in their constraints, yet both they should be distant from both A and B.
- Variety E should be different still from the others, as it was created from a combination of predictors from each variety 

To compare varieties at the level of the community, we aggregated over the data from the 15 individuals in each variety, and fit a single model per variety. To create realistic datasets, we down-sampled the data by randomly sampling 150 tokens from each speaker in each variety to give us 5 datasets of 2250 observations (150 tokens x 15 speakers) each. We account for speaker variability by including a by-speaker intercept in our model structure. 

The pairwise distance matrix for **Line 1** is shown in @tbl-line1-dist. Recall that this line considers whether the same predictors are significant across varieties, and smaller distances reflect greater cross-variety consistency in the predictor significance. In terms of the relative degree of similarity among varieties, we find that varieties C and D are identical to one another, and closer to A and B than to E. A and B are relatively dissimilar to one another, and A and E represent the two most dissimilar varieties. The actu

```{r eval = F}
brm_line1$signif.table[-1,] %>% 
  `names<-`(LETTERS[1:5]) %>% 
  kable(
    label = "line1-sig",
    caption = "Predictor significance across the 5 variety models (1 = significant)"
  ) %>%
  kable_styling("hover")
```

```{r}
#| label: tbl-line1-dist
#| tbl-cap: "Pairwise variety distance matrix as estimated by Line 1 (statistical significance)"
d <- brm_line1$distance.matrix %>% 
  round(3)
attr(d, "Labels")  <- LETTERS[1:5]
m <- as.matrix(d) 
m[upper.tri(m)] <- ""
kable(m) %>%
  kable_styling("hover")
```


From @tbl-line1-dist we calculate the similarity scores by taking the average pairwise distance for each variety and subtracting 1 (@tbl-line1-crossVar).  Based on this table we can see that Variety E is on average the most distinct variety according to the first line of evidence.  

```{r line1-crossVar, out.width="60%"}
#| label: tbl-line1-crossVar
#| tbl-cap: "Average Similarity scores across varieties as estimated by Line 1 (statistical significance)"
#| out-width: "60%"
brm_line1$similarity.scores %>% 
  mutate(
    Variety = LETTERS[1:5],
    `Mean Similarity` = round(Similarity, 3)
    ) %>% 
  select(-1) %>%
  kable() %>%
  kable_styling("hover")
```

We next turn to **Line 3**, which considers correlations among the predictor importance rankings across the varieties (@tbl-line1-dist). The first thing to note is that the absolute distances are larger for this line than those derived for Line 1. This is due to the greater amount of information in the table of rankings from which  the distances are calculated.

```{r}
#| label: tbl-line3-dist
#| tbl-cap:  "Pairwise variety distance matrix as estimated by Line 3 (predictor importance)"
d <- rm_line3$distance.matrix %>% 
  round(3)
attr(d, "Labels")  <- LETTERS[1:5]
m <- as.matrix(d) 
m[upper.tri(m)] <- ""
kable(m) %>%
  kable_styling("hover")
```

Curiously, we find that Variety E is not the most dissimilar variety, but rather D is. Overall, the scores are generally lower than for Line 1, but this is to be expected given the greater variability in the rankings.

```{r}
#| label: tbl-line3-crossVar
#| tbl-cap: "Average Similarity scores across varieties as estimated by Line 3 (predictor importance)"
rm_line3$similarity.scores %>% 
  mutate(
    Variety = LETTERS[1:5],
    `Mean Similarity` = round(Similarity, 3)
    ) %>% 
  select(-1) %>% 
  kable() %>%
  kable_styling("hover")
```

Now consider **Line 2**, which considers correlations among the predictor effect sizes, measured as the regression model coefficients (Table @tbl-line2-dist)). The first thing to note is that the distance scores for some comparisons are greater than 1, which leads to negative similarity scores as seen in Table @tbl-line2-crossVar).

```{r}
#| label: tbl-line2-dist
#| tbl-cap: "Pairwise variety distance matrix as estimated by Line 2 (effect size/direction)"
d <- brm_line2$distance.matrix %>% 
  round(3)
attr(d, "Labels")  <- LETTERS[1:5]
m <- as.matrix(d) 
m[upper.tri(m)] <- ""
kable(m) %>%
  kable_styling("hover")
```

```{r}
#| label: tbl-line2-crossVar
#| tbl-cap: "Average Similarity scores across varieties as estimated by Line 2 (effect size/direction)"
brm_line2$similarity.scores %>% 
  mutate(
    Variety = LETTERS[1:5],
    `Mean Similarity` = round(Similarity, 3)
    ) %>% 
  select(-1) %>% 
  kable() %>%
  kable_styling("hover")
```

The reason for this is that the 'maximal reasonable distance' weighting applied to the distance matrix during normalization [@szmrecsanyi_variationbased_2019, 5] is actually smaller than the maximum distances in the simulated data. In other words, the simulated varieties here are in fact much more dissimilar to one another than we'd theoretically expect to see in real data from related dialects.[^diss]

[^diss]: It remains to be seen whether this hypothesis turns out to be true. For now at least, we are unaware of real world cases where such extreme differences can be found.

To makes sense of our simulated data we then have two options. The first is to simply interpret the similarity scores as is. Since similarity is measured as 1 - the (average) distance, we can interpret the negative values along the same continuum. Lower negative values reflect lower average similarity and positive values indicate greater average similarity, and 0 is not necessarily meaningful. Based on this we can see that variety B is the least similar to the others, while D and C are the most similar.

Alternatively, the distance matrix weighting can be increased to keep the distance values between 0 and 1 (in accord with Lines 1 and 3). Table @tbl-line2-distB) shows the distance matrix with a weighting of 3, and the corresponding similarity scores in Table @tbl-line2-crossVarB). Note that the same relative trends hold between the similarity scores in Table @tbl-line2-crossVar) and Table @tbl-line2-crossVarB), B is still the least similar and D the most similar, only the absolute values have changed.

```{r echo=T, eval =F}
#| echo: true
#| eval: false 
brm_line2b <- VADIS::vadis_line2(brm_list, path = F, weight = 3)
```


```{r}
#| label: tbl-line2-distB
#| tbl-cap: "Pairwise variety distance matrix as estimated by Line 2 (effect size/direction). Weighting adjusted to 3."
brm_line2b <- VADIS::vadis_line2(brm_list, path = F, weight = 3)

d <- brm_line2b$distance.matrix %>% 
  round(3)
attr(d, "Labels")  <- LETTERS[1:5]
m <- as.matrix(d) 
m[upper.tri(m)] <- ""
kable(m) %>%
  kable_styling("hover")
```

```{r}
#| label: tbl-line2-crossVarB
#| tbl-cap: "Average Similarity scores across varieties as estimated by Line 2 (effect size/direction). Weighting adjusted to 3."
brm_line2b$similarity.scores %>% 
  mutate(
    Variety = LETTERS[1:5],
    `Mean Similarity` = round(Similarity, 3)
    ) %>% 
  select(-1) %>% 
  kable() %>%
  kable_styling("hover")
```

The differences here highlight the sensitivity of these scores to additional degrees of freedom in the modeling process, which we are currently working to reduce. While the relative similarities among the varieties is stable, the values may change quite a bit.

Finally, we turn to the combined evidence from all three lines (Table @tbl-comb-dist)). Here we can see that as planned, varieties C and D are extremely similar to one another, while the other varieties are much more distant.

```{r}
#| label: tbl-comb-dist
#| tbl-cap: "Fused pairwise variety distance matrix as estimated by all 3 Lines of evidence combined. Weighting for Line 2 = 1."
fused_d <- analogue::fuse(
  brm_line1$distance.matrix,
  brm_line2$distance.matrix,
  rm_line3$distance.matrix
) %>% round(3)
attr(fused_d, "Labels")  <- LETTERS[1:5]
m <- as.matrix(fused_d)
m[upper.tri(m)] <- ""
kable(m) %>%
  kable_styling("hover")
```


```{r}
#| label: tbl-comb-crossVarB
#| tbl-cap: "Average Similarity scores across varieties as estimated by all 3 Lines of evidence combined."
#| out-width: "60%"
m <- as.matrix(fused_d)
diag(m) <- NA
data.frame(
    Variety = LETTERS[1:5],
    `Mean Similarity` = 1 - colMeans(m, na.rm = T)
    ) %>% 
  select(-1) %>% 
  kable() %>%
  kable_styling("hover")
```

The average similarity scores are largely even, with C and D again being slightly more similar on average, though their slight bump here is due almost entirely to their high degree of similarity with one another. Based on the conclusions from the within variety scores in Section 5.1, we would be justified in concluding that based on their pairwise distance of .06 (Similarity = 1 - .06 = .94), the variable grammars of Varieties C and D are not meaningfully different from one another. 

At the same time, it is not yet clear how to interpret values below .85 or so. The distances obtained through the combined lines of evidence in Table @tbl-comb-dist) suggest that distances close to .5 - .6 reflect a very low degree of similarity, however this is *only applicable to the combined lines*. Distances from the individual lines can vary considerably in their absolute values, and we therefore tentatively suggest that comparing scores is only meaningful based on the combined lines (if at all). 

## Stepwise similarity


```{r}

```

Under construction...


# Representing uncertainty

The final issue we want to address is the inherent uncertainty in the models used to estimate the variable grammars, and how such uncertainty might be represented in the VADIS output. The standard VADIS method uses regression models for Lines 1 and 2, and random forest models for Line 3. As we've seen, the usefulness of Line 1 for visualizing distances is quite limited, and is obviously correlated with Line 2, so we will not focus on it here. With Line 2, the outputs of standard regression model tools already provide measures of uncertainty (confidence), e.g. as standard errors for the coefficient estimates. The challenge is how to incorporate that information into our representations. With Line 3, however, it's not obvious how to derive uncertainty estimates from the random forest variable importance rankings, except perhaps through bootstrapping or cross-validation, and so we set this aside for further investigation.   

An advantage of using Bayesian regression models is that we can straightforwardly sample sets of coefficients from the posterior distributions and use these to represent the variability in the model estimates.[^lme4] To do this, we randomly sample sets of coefficient estimates directly from the posterior distributions, and for each sample compute a distance matrix and unique set of MDS coordinates mapping out our 'varieties' in 2 or 3 dimensional space. These coordinates can then simply be overlaid onto one another in a single graphic representation to create 'variety clouds' (or ellipses), whose density and degree of overlap reflect the degree of uncertainty in the relative similarities among the varieties. 

To test this we use the same down-sampled variety datasets used in Section 5.2 above, and run the VADIS analysis on them. @fig-posterior-mds1 and @fig-posterior-mds2 show MDS plots of Line 2 distance matrices calculated from 200 sets of coefficients which were randomly sampled from the posterior distributions of the five variety models. 

```{r}
brm_samples <- VADIS::create_mds_samples(brm_list, path = F)
levels(brm_samples$variety) <- LETTERS[1:5]
```

```{r}
#| label: fig-posterior-mds1
#| fig-cap: "MDS plots of similarities among varieties based on 200 samples  Line 2 (effect size). Points represent individal samples from the GLMM posterior distribution with ellipses representing 50% (solid) and 90% (dashed) multivariate normal distributions."
#| fig-width: 9
#| fig-height: 9
p1 <- brm_samples %>% 
  ggplot(aes(x, y, color = variety)) +
  geom_point(alpha = .4) +
  stat_ellipse(level = .9, linetype = "dashed", type = "norm") +
  stat_ellipse(level = .5, size = 1, type = "norm") +
  ggtitle("Dimensions 1 x 2") +
  scale_color_manual(guide = "none", values = cols)

p2 <- brm_samples %>% 
  ggplot(aes(x, z, color = variety)) +
  geom_point(alpha = .4) +
  stat_ellipse(level = .9, linetype = "dashed", type = "norm") +
  stat_ellipse(level = .5, size = 1, type = "norm") +
  ggtitle("Dimensions 1 x 3") +
  scale_color_manual(guide = "none", values = cols)

p3 <- brm_samples %>% 
  ggplot(aes(y, z, color = variety)) +
  geom_point(alpha = .4) +
  stat_ellipse(level = .9, linetype = "dashed", type = "norm") +
  stat_ellipse(level = .5, size = 1, type = "norm") +
  ggtitle("Dimensions 2 x 3") +
  scale_color_manual(guide = "none", values = cols)

p1 + p2 + p3 + guide_area() + plot_layout(guides = "collect") + 
  plot_annotation(tag_levels = 'A', tag_suffix = ".") & 
  theme(plot.tag = element_text(hjust = 0)) 
```

```{r}
#| label: fig-posterior-mds2
#| fig-cap: "£D MDS plot of similarities among varieties based on 200 samples  Line 2 (effect size). Points represent individal samples from the GLMM posterior distribution."
#| fig-width: 7
#| fig-height: 6
names(cols) <- levels(brm_samples$variety)

brm_samples %>% 
  plot_ly() %>%
  add_trace(
    x = ~x, y = ~y, z = ~z,
    type = "scatter3d", inherit = F,
    color = ~ variety,
    colors = cols,
    size = ~ rep(1, 1000),
    marker = list(symbol = 'circle', sizemode = 'diameter'), sizes = c(4))
```

The tight clusters show that the coordinates from the MDS plots are highly stable, and similarities among the simulated varieties derived from the model statistics are very robust. This provides a clearly successful proof of concept for the VADIS method and the usefulness of the visualizations it can produce. Of course, real data are never so neat, and considerable work remains to test to what extent the method can be successfully applied to actual case studies.    

In addition to the MDS plots we can look at the distributions of average similarity scores across these samples. We find that as in the MDS plot, in this case variety B consistently shows the lowest average similarity, while varieties C and D are considerably more similar to one another on average.

```{r}
sample_list <- VADIS::get_posterior_samples(brm_list, n = 200)
```

```{r}
#| label: fig-posterior-sim
#| fig-cap: "Distributions of the mean similarity scores across the 5 varieties for 200 sets of coefficients randomly sampled  from the GLMM posteriors. Higher values reflect greater average grammatical similarity of the variety with all the others."
#| fig-width: 7
#| fig-height: 6
dist_list <- vector("list")
ncoef <- nrow(sample_list[[1]]) ## number of coefficients in the model(s)
nvar <- length(sample_list) ## number of varieties
coefs <- rownames(sample_list[[1]])
vars <- names(sample_list) ## names of varieties

## Loop through list of samples
dist_list <- vector("list")
for(i in seq_len(200)) {
  cur_df <- data.frame(matrix(ncol = ncoef, nrow = nvar))
  for (j in seq_along(sample_list)){
      cur_df[j, ] <- sample_list[[j]][,i]
    }
  rownames(cur_df) <- vars
  names(cur_df) <- coefs
  ## Create distance matrix
  cur_dist <- dist(cur_df, "euclidean") 
  
  cur_m <- as.matrix(cur_dist)
  diag(cur_m) <- NA # remove diagonals before calculating means
  means <- colMeans(cur_m, na.rm = T)
  ## Get the maximum reasonable distance
  # dmy <- data.frame(a = sample(c(1,-1), size = ncol(cur_df), replace = T))
  # dmy$b <- -dmy$a # exact opposite of a
  # maxD <- max(dist(t(dmy), "euclidean"))
  #
  # ## Create weighted distance matrix
  # cur_dist_wt <- cur_dist/maxD
  dist_list[[i]] <- 1 - means
}
names(dist_list) <- paste("samp", 1:200, sep = ".")
d <- bind_rows(dist_list)
names(d) <- LETTERS[1:5]

library(ggridges)
d %>% 
  pivot_longer(cols = everything()) %>% 
  mutate(name = factor(name, levels = rev(LETTERS[1:5]))) %>% 
  ggplot(aes(x = value, y = name)) +
  geom_text(label = "|") +
  geom_density_ridges(aes(fill = name), alpha = .5) +
  scale_fill_manual(guide = "none", values = cols) +
  labs(x = "Mean similarity score", y = "")
```

The findings above largely corroborate those of the MDS plots.

# Conclusion

This simulation study was motivated by three research questions regarding the validity and reliability of the new VADIS method:

**RQ 1.** How valid are the measures and representations of grammatical distances? In other words, do the measures of similarity obtained via the VADIS method accurately reflect the true degree of similarity among grammars?  

**RQ 2.** Can we determine a reasonable range of small, medium, or large degrees of grammatical similarity?

**RQ 3.** Can we capture the uncertainty in our underlying grammatical models in the VADIS output?

In answer to the first question, we find that the method is capable of accurately capturing genuine grammatical differences partly through the similarity scores, but mainly through the visual representations in the MDS plots. While the method does not provide tests of statistical significance, it can provide a useful way of exploring  grammatical similarity among varieties in a more holistic fashion. Regarding the second question, we find that distances below .1 (or Similarity scores > .9) represent practically indistinguishable variety grammars. We can also make a tentative case for scores of .5 as representing a very low degree degree of similarity, though we caution that this applies only to scores based on the **combined** lines of evidence. Lastly, we show that the inherent uncertainty in the underlying predictive models can in principle be represented in the visualizations and similarity scores via MDS 'variety clouds' and the distributions of similarity scores. 

In sum, the simulation study here demonstrate the potential value of the VADIS method as a unique and poweful tool for interpretation in comparative variationist analysis. We find that, at least in principle, the method is offers a reliable approach to exploring grammatical similarity among different variety's linguistic variables. We also show that the method can easily scale up to compare dozens of varieties (dialects, individuals,...) at a time. Further testing and application in real world contexts is surely needed, and while the study here considers only similarity with respect to a single variable, it is possible, and desirable, that the method could be expanded/adapted to explore lectal coherence across multiple linguistic variables at a time [see e.g. @guy_cognitive_2013; @oushiro_effect_2015; @szmrecsanyi_variationbased_2019], and this is an avenue that we are actively pursuing.  


[^note1]: Mixed-effects methods for random forests are not fully functional at the moment, so we cannot model the kinds of multilevel variation with random forests that we can with mixed-effects regression models. Therefore the random forest model structure is usually equivalent to the fixed-effects structure of the corresponding regression model.

[^rfnote]: We use the `{ranger}` package mainly for its extremely fast implementation, and we think this is reasonable given that the simulated datasets were designed in a way to minimize multicollinearity, which can impact the reliability of importance measures. Other random forest methods, e.g. those implemented in the `{party}` package, are arguably more appropriate due to the well-known issues of multicollinearity in natural language data, but this remains an area of some contention [@gries_classification_2019]. Practically speaking however, the very large computational cost of calculating conditional variable importance scores with `{party}` becomes prohibitive as the number of comparisons increases beyond even a few datasets (we fit 75 models here). At the moment, further testing of different methods on real language data is needed to identify the strengths and weaknesses of different random forest methods for variationist studies.

[^sig]: The `{VADIS}` package provides several other methods for assessing "significance" in Bayesian models, such as the probability of direction, Region of Practical Equivalence (ROPE), and Maximum A Posteriori (MAP) based *p*-values [@makowski_indices_2019].

[^note4]: This is effectively a modern adaptation of the notion of the 'constraint hierarchy' in traditional Comparative Sociolinguistics methods [@tagliamonte_comparative_2013], which developed out the longstanding VARBRUL tradition [@cedergren_variable_1974; @sankoff_goldvarb_2015].

[^lme4]: We are experimenting with methods for working with non-Bayesian models, e.g. by sampling from simulated parameter distributions based the coefficient point estimates and their standard errors, but this has not been implemented yet.

# R code {#sec-rcode}

Full code for replicating the simulation can be found in `vadis_simulation_code.R`.



# References
